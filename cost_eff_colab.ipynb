{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_colab.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1MmJey6VdAO"
      },
      "source": [
        "# Cost-efficient use of BERT embeddings in 8-way emotion classification on a Hungarian media corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjqQWMkPVi1v"
      },
      "source": [
        "Accompanying script for the project found at [this GitHub Repo.](https://github.com/poltextlab/Cost-efficient-use-of-BERT-in-sentiment-classification)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5ggw071VucQ"
      },
      "source": [
        "This project needs a GPU-supported Colab Notebook. After setting up, the first thing to check is whether the GPU is working as desired and the exact GPU we were assigned. It most probably will be a K80. Anything will do unless it is a P4, in which case a restart is needed as batch sizes must go very low."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qZUIZ4gwNUn"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name() # the output should be /device:GPU:0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7g2W4uPwhvk"
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices() # the last row holds the key, after 'name:'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3m_mhft2W4uE"
      },
      "source": [
        "An advantage of using Google Colab is the possibility of using Google Drive as a storage. You can find more info about this [here.](https://colab.research.google.com/notebooks/io.ipynb) This code will connect to your Drive root."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4k42wMbUQeiM"
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqpzCX_4xVhL"
      },
      "source": [
        "!pip install transformers\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning) # this is needed because of sklearn\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "import json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iN1o5LhDXo5X"
      },
      "source": [
        "The corpus needs to be in tsv format, one row per text, UTF-8 formatting. It can contain any number of columns, but \"text\" for text and \"topik\" for class are compulsory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hkyt3n-uSCHd"
      },
      "source": [
        "corpus=pd.read_csv('gdrive/My Drive/etl.tsv', sep='\\t') # this accesses the corpus from Drive root, named etl.tsv in this example"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Inbsu-xiPzt2"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"SZTAKI-HLT/hubert-base-cc\")\n",
        "model = AutoModel.from_pretrained(\"SZTAKI-HLT/hubert-base-cc\", output_hidden_layers = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPw9CTe5oi-F"
      },
      "source": [
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # this line assigns our GPU to the variable \"device\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKP1PcFESUu4"
      },
      "source": [
        "tokenized = corpus[\"text\"].apply((lambda x: tokenizer.encode(x, add_special_tokens=True))) # this creates the tokenized version of the corpus\n",
        "print(tokenized)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zC5FrT8Y6zM"
      },
      "source": [
        "Max length (here: max_len) is the length of the longest token sequence in the corpus. In this example it is presumed that it is less than 510. If more, you need to cut off anything longer than 510.\n",
        "\n",
        "It might be advisable to look at the distribution of lengths in the corpus, as if there are only a few lines with very long length, it is sensible to prune them to fit the majority of the corpus. If need be, the tokenizer.encode can truncate to a specified length."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRl7K9lgYsjD"
      },
      "source": [
        "max_len = 0\n",
        "for i in tokenized.values:\n",
        "    if len(i) > max_len:\n",
        "        max_len = len(i)\n",
        "print('Max length is:', max_len)\n",
        "\n",
        "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\n",
        "attention_mask = np.where(padded != 0, 1, 0)\n",
        "batchsize = (len(corpus) // 200) + 1 # the size of the batches can be manipulated by modifying the divisor here\n",
        "print('Number of batches:', batchsize)\n",
        "\n",
        "splitpadded = np.array_split(padded, batchsize)\n",
        "splitmask = np.array_split(attention_mask, batchsize)\n",
        "\n",
        "last_hidden_states = []\n",
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwAsZg3UaOFG"
      },
      "source": [
        "The model provides contextual embeddings from the tokens. They are saved as a numpy ndarray, after being averaged across the hidden states. The variable *all_hidden_states* is the output from the model, containing all tensors, while *featuresfinal* is the object containing the embeddings. These are reached by slicing the CLS token from all hidden layer outputs and averaging them across layer positions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1i8Wg99kSg_U"
      },
      "source": [
        "def last_hidden():\n",
        "    for count, i in enumerate(splitpadded):\n",
        "        paddedsplit = np.array(i, dtype='float64')\n",
        "        length = len(paddedsplit)\n",
        "        input_batch = torch.tensor(i).to(torch.long)\n",
        "        mask_batch = torch.tensor(attention_mask[length*count:length*count+length])\n",
        "        input_batch = input_batch.to(device)\n",
        "        mask_batch = mask_batch.to(device)\n",
        "        # no_grad ensures there is no gradient update in the model, as we are not looking for recursive training here\n",
        "        with torch.no_grad():\n",
        "            global all_hidden_states\n",
        "            all_hidden_states = model(input_batch, attention_mask=mask_batch)\n",
        "        print('Hidden states created for batch', count+1)\n",
        "        global features\n",
        "        hs12 = all_hidden_states.hidden_states[0][:,0,:].cpu()\n",
        "        hs11 = all_hidden_states.hidden_states[1][:,0,:].cpu()\n",
        "        hs10 = all_hidden_states.hidden_states[2][:,0,:].cpu()\n",
        "        hs9 = all_hidden_states.hidden_states[3][:,0,:].cpu()\n",
        "        hs8 = all_hidden_states.hidden_states[4][:,0,:].cpu()\n",
        "        hs7 = all_hidden_states.hidden_states[5][:,0,:].cpu()\n",
        "        hs6 = all_hidden_states.hidden_states[6][:,0,:].cpu()\n",
        "        hs5 = all_hidden_states.hidden_states[7][:,0,:].cpu()\n",
        "        hs4 = all_hidden_states.hidden_states[8][:,0,:].cpu()\n",
        "        hs3 = all_hidden_states.hidden_states[9][:,0,:].cpu()\n",
        "        hs2 = all_hidden_states.hidden_states[10][:,0,:].cpu()\n",
        "        hs1 = all_hidden_states.hidden_states[11][:,0,:].cpu()\n",
        "        concat_tensor = tf.stack([hs12, hs11, hs10, hs9, hs8, hs7, hs6, hs5, hs4, hs3, hs2, hs1], axis = 1)\n",
        "        final_tensor = tf.reduce_mean(concat_tensor, axis=1).numpy()\n",
        "        global featuresfinal\n",
        "        featuresfinal = np.append(featuresfinal, final_tensor, axis=0)\n",
        "        print('Finished with batch', count+1)\n",
        "\n",
        "print('Model is running on', model.device) # one last check for a proper GPU-run\n",
        "last_hidden()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DHJ0FT-a2l4"
      },
      "source": [
        "At this point it is advisable to save the *featuresfinal* and the *labels* objects, if the corpus will stay the same later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5t_N-LoT6h6"
      },
      "source": [
        "from google.colab import files\n",
        "np.save(\"featuresfinal\", featuresfinal)\n",
        "!cp featuresfinal.npy \"/content/gdrive/My Drive/\"\n",
        "np.save(\"labels\", corpus[\"topik\"])\n",
        "!cp labels.npy \"/content/gdrive/My Drive/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzZ7kEUUbNnn"
      },
      "source": [
        "If you already have your files ready, you can load them with this snippet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "webh7gI-y7hk"
      },
      "source": [
        "#featuresfinal = np.load(\"/content/gdrive/My Drive/featuresfinal.npy\")\n",
        "#labels = np.load(\"/content/gdrive/My Drive/labels.npy\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iANZuz0PVsZR"
      },
      "source": [
        "# MinMax scaling is applied to the features\n",
        "scaler = MinMaxScaler()\n",
        "featuresfinal = scaler.fit_transform(featuresfinal)\n",
        "\n",
        "# the parameter space is defined below\n",
        "C = [0.1, 1]\n",
        "tol = [0.001, 0.005, 0.01]\n",
        "weighting = ['balanced']\n",
        "solver = ['liblinear']\n",
        "max_iter = [6000]\n",
        "parameters = dict(C=C, tol=tol, class_weight=weighting, solver=solver, max_iter=max_iter)\n",
        "\n",
        "# Necessary objects and variables are initialized\n",
        "clasrep = list()\n",
        "paramlist = list()\n",
        "labels = corpus[\"topik\"].to_numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vg-2qqcbosy"
      },
      "source": [
        "The logistic regression below splits and fits again every iteration, and appends results as dicts to list *clasrep*, and best parameters to list *paramlist*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgMVHdpfWKLq"
      },
      "source": [
        "for i in range(3):\n",
        "    train_features, test_features, train_labels, test_labels = train_test_split(featuresfinal, labels, stratify=labels)\n",
        "    lr = LogisticRegression()\n",
        "    lrmodel = GridSearchCV(lr, parameters, cv = 3, scoring = 'f1_weighted', n_jobs = -1)\n",
        "    lrmodel.fit(train_features, train_labels)\n",
        "    predictions = lrmodel.predict(test_features)\n",
        "    classifrep = classification_report(test_labels, predictions, output_dict = True)\n",
        "    clasrep.append(classifrep)\n",
        "    paramlist.append(lrmodel.best_params_)\n",
        "    print(\"Finished with run!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADBiI483b4Zy"
      },
      "source": [
        "The code below exports the resulting lists of dicts as JSON files, then copies them to Drive root."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTQGOY5PX31o"
      },
      "source": [
        "MyFile = open('clasrep_bert.json', 'w')\n",
        "json.dump(clasrep, MyFile)\n",
        "MyFile.close()\n",
        "\n",
        "MyFile = open('param_bert.json', 'w')\n",
        "json.dump(paramlist, MyFile)\n",
        "MyFile.close()\n",
        "\n",
        "!cp clasrep_bert_three.json \"/content/gdrive/My Drive/\"\n",
        "!cp param_bert_three.json \"/content/gdrive/My Drive/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxIQ25TmtyIK"
      },
      "source": [
        "results = pd.io.json.json_normalize(clasrep) # parsing through the lists as if they were JSONs\n",
        "results.mean() # weighted precision, recall, F1 scores and sample sizes averaged over the runs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aS7oumNIgdCv"
      },
      "source": [
        "From this point, one has the option to use the total labeled set and predict for unlabeled data after tokenizing them and extracting their contextual embeddings. Sklearn also offers .predict_proba() method for probabilistic output for greater granularity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imPjwN6BcUZh"
      },
      "source": [
        "lrmodel.predict(unlabeled_features)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
